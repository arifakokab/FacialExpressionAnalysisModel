{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "811db78e-25c2-4fd8-900a-53bc8661fa27",
   "metadata": {},
   "source": [
    "---\n",
    "**Author:** Arifa Kokab  \n",
    "**For:** AAI-540 Machine Learning Operations  \n",
    "**Institution:** University of San Diego"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af2034d-d637-4ed3-ae48-087accd12b51",
   "metadata": {},
   "source": [
    "# Batch Inference with SageMaker Batch Transform\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to perform batch inference on a set of extracted video frames using an already registered and deployed facial expression analysis model in AWS SageMaker. Batch Transform enables large-scale, asynchronous prediction, supporting scalable and efficient emotion recognition across video data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58361db-083b-4cf9-9902-e96170e80d11",
   "metadata": {},
   "source": [
    "## 1. Setup: Import Libraries and Initialize SageMaker Session\n",
    "\n",
    "This section imports the necessary SageMaker and AWS libraries, sets up the execution role and session, and defines input/output S3 locations for the batch job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "641388e7-40cf-461e-8449-1c600dcc5060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T08:18:19.469561Z",
     "iopub.status.busy": "2025-06-26T08:18:19.469236Z",
     "iopub.status.idle": "2025-06-26T08:18:22.226796Z",
     "shell.execute_reply": "2025-06-26T08:18:22.226135Z",
     "shell.execute_reply.started": "2025-06-26T08:18:19.469537Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role, Session\n",
    "from sagemaker.transformer import Transformer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a92670f3-19c0-46b8-a1f5-a1daff1f226e",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-06-26T08:18:31.830739Z",
     "iopub.status.busy": "2025-06-26T08:18:31.830234Z",
     "iopub.status.idle": "2025-06-26T08:18:32.123317Z",
     "shell.execute_reply": "2025-06-26T08:18:32.122671Z",
     "shell.execute_reply.started": "2025-06-26T08:18:31.830712Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'FacialExpressionAnalysisFinal'\n",
    "input_location = 's3://sagemaker-us-east-1-301806113644/batch_input/'   # Folder with input frames\n",
    "output_location = 's3://sagemaker-us-east-1-301806113644/batch_output/' # Folder for results\n",
    "\n",
    "role = get_execution_role()\n",
    "sagemaker_session = Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417433ba-ab3a-482b-b608-23069c456e7f",
   "metadata": {},
   "source": [
    "## 2. Launch Batch Transform Job\n",
    "\n",
    "We create and configure a SageMaker `Transformer` to process all input frames in batch mode, saving the predicted emotion results to S3. This approach scales well for high-throughput inference workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee74dbd6-bdc1-496c-89e6-cbc3b1138bea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T08:18:37.343127Z",
     "iopub.status.busy": "2025-06-26T08:18:37.342783Z",
     "iopub.status.idle": "2025-06-26T08:18:37.498724Z",
     "shell.execute_reply": "2025-06-26T08:18:37.498125Z",
     "shell.execute_reply.started": "2025-06-26T08:18:37.343103Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create the transformer\n",
    "transformer = Transformer(\n",
    "    model_name=model_name,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path=output_location,\n",
    "    assemble_with='Line',\n",
    "    accept='application/json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "426bf4dc-a494-4328-9366-4cc0b103b5b9",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-06-26T08:18:43.362823Z",
     "iopub.status.busy": "2025-06-26T08:18:43.362486Z",
     "iopub.status.idle": "2025-06-26T08:25:18.276398Z",
     "shell.execute_reply": "2025-06-26T08:25:18.275715Z",
     "shell.execute_reply.started": "2025-06-26T08:18:43.362799Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: pytorch-inference-2025-06-26-08-18-43-515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 1)) (1.9.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 2)) (0.10.1+cpu)\u001b[0m\n",
      "\u001b[34mCollecting timm\n",
      "  Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 63.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 4)) (9.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->-r /opt/ml/model/code/requirements.txt (line 1)) (4.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision->-r /opt/ml/model/code/requirements.txt (line 2)) (1.23.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.33.1-py3-none-any.whl (515 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 515.4/515.4 kB 47.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 471.6/471.6 kB 45.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from timm->-r /opt/ml/model/code/requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting packaging>=20.9\n",
      "  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.5/66.5 kB 10.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting hf-xet<2.0.0,>=1.1.2\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 92.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.6/193.6 kB 27.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.8/site-packages (from huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (4.64.0)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (2.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (2.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (1.26.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (2022.6.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (3.3)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: safetensors, packaging, hf-xet, fsspec, filelock, huggingface_hub, timm\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.4\n",
      "    Uninstalling packaging-20.4:\n",
      "      Successfully uninstalled packaging-20.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed filelock-3.16.1 fsspec-2025.3.0 hf-xet-1.1.5 huggingface_hub-0.33.1 packaging-25.0 safetensors-0.5.3 timm-1.0.15\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 25.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,208 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,307 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.6.0\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 2\u001b[0m\n",
      "\u001b[34mMax heap size: 970 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.8\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model=/opt/ml/model\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 2\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mLimit Maximum Image Pixels: true\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34mWorkflow Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mModel config: N/A\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,318 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,353 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,360 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,360 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,363 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,380 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,632 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,633 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,639 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,391 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,558 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,559 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:46.573604583740234|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,560 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:9.291526794433594|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,560 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:16.6|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,561 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6562.7421875|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,562 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:891.125|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,562 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:15.3|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,678 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,681 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]51\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,684 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,686 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.10\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,689 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,726 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,731 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926262731\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,830 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,832 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,833 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]52\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,834 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,834 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,836 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.10\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,850 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,854 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926262854\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,956 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:24,072 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1246\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:24,073 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1118\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:24,072 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1246\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:24,073 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1118\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:24,073 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:2700|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926264\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:24,074 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:102|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926264\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:24,073 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:2702|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926264\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:24,075 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:98|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926264\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:24,073 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:2700|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926264\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:24,074 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:102|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926264\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:24,073 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:2702|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926264\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:24,075 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:98|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926264\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:28,335 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:40582 \"GET /ping HTTP/1.1\" 200 32\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:28,336 [INFO ] pool-2-thread-3 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:28,383 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:40584 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:28,385 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:28,335 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:40582 \"GET /ping HTTP/1.1\" 200 32\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:28,336 [INFO ] pool-2-thread-3 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:28,383 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:40584 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:28,385 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,025 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926269025\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,025 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926269025\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,032 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926269\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,210 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 179\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,211 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 207\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,212 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,212 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926269\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,213 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:178.3|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:90879909-1323-4b10-81a1-d4cc0237f4df,timestamp:1750926269\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,214 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926269\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,444 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926269444\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,449 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,032 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,210 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 179\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,211 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 207\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,212 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,212 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,213 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:178.3|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:90879909-1323-4b10-81a1-d4cc0237f4df,timestamp:1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,214 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,444 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926269444\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,449 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926269\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,613 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 165\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,613 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 165\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,614 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 176\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,616 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,613 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:162.89|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:a39eed58-87c2-45ff-96a2-434ba03f3663,timestamp:1750926269\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,617 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926269\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,622 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:13|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926269\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,931 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926269930\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,933 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926269\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,073 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:138.73|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:9e9a4810-780a-4100-a5b5-bdf81ce12a4e,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,073 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 140\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,075 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 155\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,076 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,077 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,077 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,303 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926270302\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,305 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,434 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 129\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,435 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 145\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,437 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,437 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,439 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,440 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:127.44|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:bad60d8f-3e2b-426d-a841-7a418563ebb0,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,614 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 176\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,616 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,613 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:162.89|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:a39eed58-87c2-45ff-96a2-434ba03f3663,timestamp:1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,617 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,622 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:13|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,931 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926269930\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,933 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,073 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:138.73|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:9e9a4810-780a-4100-a5b5-bdf81ce12a4e,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,073 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 140\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,075 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 155\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,076 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,077 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,077 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,303 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926270302\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,305 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,434 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 129\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,435 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 145\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,437 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,437 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,439 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,440 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:127.44|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:bad60d8f-3e2b-426d-a841-7a418563ebb0,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,555 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926270555\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,559 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,735 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 178\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,736 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 192\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,736 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,736 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,735 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:177.65|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:8cfe11bb-721d-46e0-8a3f-5038b619c80e,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,736 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,854 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926270854\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,862 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,972 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 114\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,972 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:112.99|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:8f84cb82-7347-4afd-8997-484f5d869cd0,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,973 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 122\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,974 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,978 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,979 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:11|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,155 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926271155\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,555 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926270555\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,559 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,735 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 178\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,736 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 192\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,736 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,736 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,735 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:177.65|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:8cfe11bb-721d-46e0-8a3f-5038b619c80e,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,736 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,854 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926270854\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,862 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,972 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 114\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,972 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:112.99|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:8f84cb82-7347-4afd-8997-484f5d869cd0,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,973 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 122\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,974 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,978 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,979 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:11|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,155 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926271155\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,157 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,271 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 115\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,272 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 128\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,273 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,273 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,274 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,275 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:113.86|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:d29a9c7e-4a20-4e37-a033-0ae533d77913,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,371 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926271371\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,374 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,157 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,271 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 115\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,272 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 128\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,273 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,273 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,274 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,275 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:113.86|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:d29a9c7e-4a20-4e37-a033-0ae533d77913,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,371 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926271371\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,374 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,499 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:123.92|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:0a8da93a-9a97-4693-be5c-bd4bf35fcd91,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,499 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 125\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,499 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:123.92|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:0a8da93a-9a97-4693-be5c-bd4bf35fcd91,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,499 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 125\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,501 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 137\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,501 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,502 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,507 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:11|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,867 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926271867\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,870 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,971 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 102\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,972 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 118\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,974 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,975 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,977 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,978 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:101.29|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:f1cfcdbc-df56-438d-a988-393a7d55b6d2,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,046 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272046\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,049 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,161 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 113\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,162 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 121\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,162 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,161 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:111.74|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:9f917897-7f13-4418-8288-025c339df7bc,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,163 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:3|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,163 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,255 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272255\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,262 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,382 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 123\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,384 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 132\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,386 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,501 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 137\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,501 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,502 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,507 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:11|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,867 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926271867\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,870 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,971 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 102\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,972 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 118\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,974 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,975 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,977 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,978 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:101.29|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:f1cfcdbc-df56-438d-a988-393a7d55b6d2,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,046 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272046\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,049 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,161 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 113\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,162 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 121\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,162 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,161 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:111.74|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:9f917897-7f13-4418-8288-025c339df7bc,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,163 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:3|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,163 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,255 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272255\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,262 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,382 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 123\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,384 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 132\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,386 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,388 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,390 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:11|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,396 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:121.93|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:84563eb3-7ca8-40fd-8578-4f8c6d05166a,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,388 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,390 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:11|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,396 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:121.93|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:84563eb3-7ca8-40fd-8578-4f8c6d05166a,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,520 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272520\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,530 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,648 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 119\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,649 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 132\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,650 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,650 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,653 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:14|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,654 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:117.74|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:19f1a9e1-07a8-4ad5-9e1f-29007ad1efdc,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,736 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272736\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,741 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,853 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 114\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,854 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 121\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,855 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,855 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,858 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,854 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:113.41|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:5a2912e6-f2d9-44e5-9a82-053f11f2b3d6,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,520 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272520\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,530 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,648 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 119\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,649 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 132\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,650 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,650 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,653 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:14|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,654 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:117.74|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:19f1a9e1-07a8-4ad5-9e1f-29007ad1efdc,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,736 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272736\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,741 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,853 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 114\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,854 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 121\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,855 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,855 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,858 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,854 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:113.41|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:5a2912e6-f2d9-44e5-9a82-053f11f2b3d6,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,943 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272943\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,947 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,069 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 123\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,069 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:121.39|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:8300b61c-b60d-428d-9889-532312cdfdf9,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,070 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 130\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,070 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,071 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,073 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,189 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926273189\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,192 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,337 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:144.25|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:7ba2dd0c-8372-4456-8dd5-c94c670bf771,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,338 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 146\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,339 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 156\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,340 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,340 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,341 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,439 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926273439\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,446 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,943 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272943\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,947 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,069 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 123\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,069 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:121.39|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:8300b61c-b60d-428d-9889-532312cdfdf9,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,070 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 130\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,070 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,071 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,073 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,189 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926273189\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,192 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,337 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:144.25|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:7ba2dd0c-8372-4456-8dd5-c94c670bf771,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,338 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 146\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,339 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 156\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,340 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,340 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,341 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,439 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926273439\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,446 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926273\u001b[0m\n",
      "\u001b[32m2025-06-26T08:24:28.404:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,043 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,044 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,042 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:115.57|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:a0470b15-b1e6-45e6-a4a9-251db29c3a7c,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,045 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:10|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,115 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926274115\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,118 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,239 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 122\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,240 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 137\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,239 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:120.29|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:86bb02f6-2943-44da-9e85-1d752f1c8ea8,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,240 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,043 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,044 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,042 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:115.57|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:a0470b15-b1e6-45e6-a4a9-251db29c3a7c,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,045 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:10|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,115 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926274115\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,118 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,239 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 122\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,240 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 137\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,239 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:120.29|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:86bb02f6-2943-44da-9e85-1d752f1c8ea8,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,240 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,240 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,241 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,350 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926274350\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,358 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,240 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,241 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,350 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926274350\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,358 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,485 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:131.51|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:cdc0b765-7029-4b71-84bf-542a69fd5aa2,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,485 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 133\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,486 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 139\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,487 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,488 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,489 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,581 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926274581\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,586 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,700 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 117\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,701 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 134\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,701 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,702 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,705 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,706 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:115.84|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:f427b933-885d-44d9-bdcd-8e21b6a3eeff,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,778 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926274778\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,786 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,916 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 135\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,919 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 153\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,919 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,919 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,920 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,923 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:133.34|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:edc6c9a2-afe5-4ba7-9598-c91a5f9ab663,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,485 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:131.51|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:cdc0b765-7029-4b71-84bf-542a69fd5aa2,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,485 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 133\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,486 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 139\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,487 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,488 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,489 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,581 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926274581\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,586 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,700 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 117\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,701 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 134\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,701 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,702 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,705 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,706 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:115.84|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:f427b933-885d-44d9-bdcd-8e21b6a3eeff,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,778 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926274778\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,786 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,916 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 135\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,919 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 153\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,919 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,919 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,920 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,923 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:133.34|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:edc6c9a2-afe5-4ba7-9598-c91a5f9ab663,timestamp:1750926274\u001b[0m\n",
      "\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 1)) (1.9.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 2)) (0.10.1+cpu)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 1)) (1.9.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torchvision in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 2)) (0.10.1+cpu)\u001b[0m\n",
      "\u001b[34mCollecting timm\n",
      "  Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 63.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 4)) (9.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->-r /opt/ml/model/code/requirements.txt (line 1)) (4.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision->-r /opt/ml/model/code/requirements.txt (line 2)) (1.23.2)\u001b[0m\n",
      "\u001b[34mCollecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.33.1-py3-none-any.whl (515 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 515.4/515.4 kB 47.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting safetensors\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 471.6/471.6 kB 45.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from timm->-r /opt/ml/model/code/requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[34mCollecting packaging>=20.9\n",
      "  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.5/66.5 kB 10.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting hf-xet<2.0.0,>=1.1.2\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 92.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.6/193.6 kB 27.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.8/site-packages (from huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (4.64.0)\u001b[0m\n",
      "\u001b[34mCollecting filelock\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (2.28.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (2.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (1.26.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (2022.6.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (3.3)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: safetensors, packaging, hf-xet, fsspec, filelock, huggingface_hub, timm\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.4\n",
      "    Uninstalling packaging-20.4:\n",
      "      Successfully uninstalled packaging-20.4\u001b[0m\n",
      "\u001b[35mCollecting timm\n",
      "  Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 63.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: Pillow in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 4)) (9.2.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->-r /opt/ml/model/code/requirements.txt (line 1)) (4.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchvision->-r /opt/ml/model/code/requirements.txt (line 2)) (1.23.2)\u001b[0m\n",
      "\u001b[35mCollecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.33.1-py3-none-any.whl (515 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 515.4/515.4 kB 47.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting safetensors\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 471.6/471.6 kB 45.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from timm->-r /opt/ml/model/code/requirements.txt (line 3)) (5.4.1)\u001b[0m\n",
      "\u001b[35mCollecting packaging>=20.9\n",
      "  Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.5/66.5 kB 10.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting hf-xet<2.0.0,>=1.1.2\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 92.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.6/193.6 kB 27.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.8/site-packages (from huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (4.64.0)\u001b[0m\n",
      "\u001b[35mCollecting filelock\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (2.28.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (2.0.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (1.26.11)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (2022.6.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->huggingface_hub->timm->-r /opt/ml/model/code/requirements.txt (line 3)) (3.3)\u001b[0m\n",
      "\u001b[35mInstalling collected packages: safetensors, packaging, hf-xet, fsspec, filelock, huggingface_hub, timm\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 20.4\n",
      "    Uninstalling packaging-20.4:\n",
      "      Successfully uninstalled packaging-20.4\u001b[0m\n",
      "\u001b[34mSuccessfully installed filelock-3.16.1 fsspec-2025.3.0 hf-xet-1.1.5 huggingface_hub-0.33.1 packaging-25.0 safetensors-0.5.3 timm-1.0.15\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 25.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35mSuccessfully installed filelock-3.16.1 fsspec-2025.3.0 hf-xet-1.1.5 huggingface_hub-0.33.1 packaging-25.0 safetensors-0.5.3 timm-1.0.15\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip available: 22.2.2 -> 25.0.1\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,208 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,307 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[34mTorchserve version: 0.6.0\u001b[0m\n",
      "\u001b[34mTS Home: /opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[35mWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:21,208 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:21,307 [INFO ] main org.pytorch.serve.ModelServer - \u001b[0m\n",
      "\u001b[35mTorchserve version: 0.6.0\u001b[0m\n",
      "\u001b[35mTS Home: /opt/conda/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mCurrent directory: /\u001b[0m\n",
      "\u001b[34mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[34mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[34mNumber of CPUs: 2\u001b[0m\n",
      "\u001b[34mMax heap size: 970 M\u001b[0m\n",
      "\u001b[34mPython executable: /opt/conda/bin/python3.8\u001b[0m\n",
      "\u001b[34mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[34mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mInitial Models: model=/opt/ml/model\u001b[0m\n",
      "\u001b[34mLog dir: /logs\u001b[0m\n",
      "\u001b[34mMetrics dir: /logs\u001b[0m\n",
      "\u001b[34mNetty threads: 0\u001b[0m\n",
      "\u001b[34mNetty client threads: 0\u001b[0m\n",
      "\u001b[34mDefault workers per model: 2\u001b[0m\n",
      "\u001b[34mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[34mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[34mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[34mLimit Maximum Image Pixels: true\u001b[0m\n",
      "\u001b[34mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[34mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[35mCurrent directory: /\u001b[0m\n",
      "\u001b[35mTemp directory: /home/model-server/tmp\u001b[0m\n",
      "\u001b[35mNumber of GPUs: 0\u001b[0m\n",
      "\u001b[35mNumber of CPUs: 2\u001b[0m\n",
      "\u001b[35mMax heap size: 970 M\u001b[0m\n",
      "\u001b[35mPython executable: /opt/conda/bin/python3.8\u001b[0m\n",
      "\u001b[35mConfig file: /etc/sagemaker-ts.properties\u001b[0m\n",
      "\u001b[35mInference address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35mManagement address: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35mMetrics address: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[35mModel Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[35mInitial Models: model=/opt/ml/model\u001b[0m\n",
      "\u001b[35mLog dir: /logs\u001b[0m\n",
      "\u001b[35mMetrics dir: /logs\u001b[0m\n",
      "\u001b[35mNetty threads: 0\u001b[0m\n",
      "\u001b[35mNetty client threads: 0\u001b[0m\n",
      "\u001b[35mDefault workers per model: 2\u001b[0m\n",
      "\u001b[35mBlacklist Regex: N/A\u001b[0m\n",
      "\u001b[35mMaximum Response Size: 6553500\u001b[0m\n",
      "\u001b[35mMaximum Request Size: 6553500\u001b[0m\n",
      "\u001b[35mLimit Maximum Image Pixels: true\u001b[0m\n",
      "\u001b[35mPrefer direct buffer: false\u001b[0m\n",
      "\u001b[35mAllowed Urls: [file://.*|http(s)?://.*]\u001b[0m\n",
      "\u001b[34mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[34mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[34mEnable metrics API: true\u001b[0m\n",
      "\u001b[34mWorkflow Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[34mModel config: N/A\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,318 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,353 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,360 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,360 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,363 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,380 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35mCustom python dependency for model allowed: false\u001b[0m\n",
      "\u001b[35mMetrics report format: prometheus\u001b[0m\n",
      "\u001b[35mEnable metrics API: true\u001b[0m\n",
      "\u001b[35mWorkflow Store: /.sagemaker/ts/models\u001b[0m\n",
      "\u001b[35mModel config: N/A\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:21,318 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:21,353 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:21,360 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:21,360 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:21,363 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:21,380 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,632 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:21,632 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,633 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:21,639 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[34mModel server started.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,391 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:21,633 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:21,639 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\u001b[0m\n",
      "\u001b[35mModel server started.\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,391 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,558 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,559 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:46.573604583740234|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,560 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:9.291526794433594|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,560 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:16.6|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,561 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6562.7421875|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,562 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:891.125|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,562 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:15.3|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,678 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,558 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:100.0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,559 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:46.573604583740234|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,560 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:9.291526794433594|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,560 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:16.6|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,561 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:6562.7421875|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,562 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:891.125|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,562 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:15.3|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926262\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,678 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,681 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]51\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,684 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,686 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.10\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,689 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,726 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,731 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926262731\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,830 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,832 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,833 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]52\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,834 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,834 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,836 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.10\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,850 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,854 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926262854\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:22,956 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,681 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]51\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,684 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,686 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.10\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,689 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,726 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,731 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926262731\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,830 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,832 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,833 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]52\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,834 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,834 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,836 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.10\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,850 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,854 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926262854\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:22,956 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:24,072 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1246\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:24,073 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1118\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:24,072 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1246\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:24,073 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 1118\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:24,073 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:2700|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926264\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:24,074 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:102|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926264\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:24,073 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:2702|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926264\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:24,075 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:98|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926264\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:24,073 [INFO ] W-9001-model_1.0 TS_METRICS - W-9001-model_1.0.ms:2700|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926264\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:24,074 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:102|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926264\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:24,073 [INFO ] W-9000-model_1.0 TS_METRICS - W-9000-model_1.0.ms:2702|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926264\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:24,075 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:98|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926264\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:28,335 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:40582 \"GET /ping HTTP/1.1\" 200 32\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:28,336 [INFO ] pool-2-thread-3 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:28,383 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:40584 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:28,385 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:28,335 [INFO ] pool-2-thread-3 ACCESS_LOG - /169.254.255.130:40582 \"GET /ping HTTP/1.1\" 200 32\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:28,336 [INFO ] pool-2-thread-3 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:28,383 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /169.254.255.130:40584 \"GET /execution-parameters HTTP/1.1\" 404 1\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:28,385 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,025 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926269025\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,025 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926269025\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,032 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926269\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,210 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 179\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,211 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 207\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,212 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,212 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926269\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,213 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:178.3|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:90879909-1323-4b10-81a1-d4cc0237f4df,timestamp:1750926269\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,214 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926269\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,444 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926269444\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,449 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,032 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,210 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 179\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,211 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 207\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,212 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,212 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,213 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:178.3|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:90879909-1323-4b10-81a1-d4cc0237f4df,timestamp:1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,214 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,444 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926269444\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,449 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926269\u001b[0m\n",
      "\u001b[32m2025-06-26T08:24:28.404:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,613 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 165\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,613 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 165\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,614 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 176\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,616 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,613 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:162.89|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:a39eed58-87c2-45ff-96a2-434ba03f3663,timestamp:1750926269\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,617 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926269\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,622 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:13|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926269\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,931 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926269930\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:29,933 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926269\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,073 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:138.73|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:9e9a4810-780a-4100-a5b5-bdf81ce12a4e,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,073 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 140\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,075 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 155\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,076 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,077 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,077 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,303 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926270302\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,305 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,434 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 129\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,435 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 145\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,437 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,437 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,439 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,440 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:127.44|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:bad60d8f-3e2b-426d-a841-7a418563ebb0,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,614 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 176\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,616 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,613 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:162.89|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:a39eed58-87c2-45ff-96a2-434ba03f3663,timestamp:1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,617 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,622 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:13|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,931 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926269930\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:29,933 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926269\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,073 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:138.73|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:9e9a4810-780a-4100-a5b5-bdf81ce12a4e,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,073 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 140\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,075 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 155\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,076 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,077 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,077 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,303 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926270302\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,305 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,434 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 129\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,435 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 145\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,437 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,437 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,439 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,440 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:127.44|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:bad60d8f-3e2b-426d-a841-7a418563ebb0,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,555 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926270555\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,559 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,735 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 178\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,736 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 192\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,736 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,736 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,735 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:177.65|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:8cfe11bb-721d-46e0-8a3f-5038b619c80e,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,736 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,854 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926270854\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,862 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,972 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 114\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,972 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:112.99|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:8f84cb82-7347-4afd-8997-484f5d869cd0,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,973 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 122\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,974 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,978 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:30,979 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:11|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,155 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926271155\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,555 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926270555\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,559 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,735 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 178\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,736 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 192\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,736 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,736 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,735 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:177.65|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:8cfe11bb-721d-46e0-8a3f-5038b619c80e,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,736 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,854 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926270854\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,862 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,972 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 114\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,972 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:112.99|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:8f84cb82-7347-4afd-8997-484f5d869cd0,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,973 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 122\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,974 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,978 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:30,979 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:11|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926270\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,155 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926271155\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,157 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,271 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 115\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,272 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 128\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,273 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,273 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,274 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,275 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:113.86|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:d29a9c7e-4a20-4e37-a033-0ae533d77913,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,371 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926271371\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,374 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,157 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,271 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 115\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,272 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 128\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,273 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,273 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,274 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,275 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:113.86|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:d29a9c7e-4a20-4e37-a033-0ae533d77913,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,371 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926271371\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,374 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,499 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:123.92|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:0a8da93a-9a97-4693-be5c-bd4bf35fcd91,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,499 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 125\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,499 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:123.92|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:0a8da93a-9a97-4693-be5c-bd4bf35fcd91,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,499 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 125\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,501 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 137\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,501 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,502 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,507 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:11|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,867 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926271867\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,870 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,971 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 102\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,972 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 118\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,974 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,975 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,977 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:31,978 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:101.29|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:f1cfcdbc-df56-438d-a988-393a7d55b6d2,timestamp:1750926271\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,046 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272046\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,049 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,161 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 113\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,162 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 121\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,162 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,161 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:111.74|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:9f917897-7f13-4418-8288-025c339df7bc,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,163 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:3|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,163 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,255 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272255\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,262 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,382 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 123\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,384 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 132\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,386 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,501 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 137\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,501 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,502 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,507 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:11|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,867 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926271867\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,870 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,971 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 102\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,972 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 118\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,974 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,975 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,977 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:31,978 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:101.29|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:f1cfcdbc-df56-438d-a988-393a7d55b6d2,timestamp:1750926271\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,046 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272046\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,049 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,161 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 113\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,162 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 121\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,162 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,161 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:111.74|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:9f917897-7f13-4418-8288-025c339df7bc,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,163 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:3|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,163 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:4|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,255 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272255\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,262 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,382 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 123\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,384 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 132\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,386 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,388 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,390 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:11|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,396 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:121.93|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:84563eb3-7ca8-40fd-8578-4f8c6d05166a,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,388 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,390 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:11|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,396 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:121.93|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:84563eb3-7ca8-40fd-8578-4f8c6d05166a,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,520 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272520\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,530 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,648 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 119\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,649 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 132\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,650 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,650 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,653 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:14|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,654 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:117.74|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:19f1a9e1-07a8-4ad5-9e1f-29007ad1efdc,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,736 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272736\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,741 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,853 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 114\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,854 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 121\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,855 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,855 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,858 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,854 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:113.41|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:5a2912e6-f2d9-44e5-9a82-053f11f2b3d6,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,520 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272520\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,530 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,648 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 119\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,649 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 132\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,650 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,650 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,653 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:14|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,654 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:117.74|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:19f1a9e1-07a8-4ad5-9e1f-29007ad1efdc,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,736 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272736\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,741 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,853 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 114\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,854 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 121\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,855 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,855 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,858 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,854 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:113.41|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:5a2912e6-f2d9-44e5-9a82-053f11f2b3d6,timestamp:1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,943 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272943\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:32,947 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,069 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 123\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,069 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:121.39|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:8300b61c-b60d-428d-9889-532312cdfdf9,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,070 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 130\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,070 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,071 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,073 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,189 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926273189\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,192 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,337 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:144.25|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:7ba2dd0c-8372-4456-8dd5-c94c670bf771,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,338 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 146\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,339 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 156\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,340 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,340 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,341 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,439 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926273439\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,446 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,943 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926272943\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:32,947 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926272\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,069 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 123\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,069 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:121.39|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:8300b61c-b60d-428d-9889-532312cdfdf9,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,070 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 130\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,070 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,071 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,073 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,189 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926273189\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,192 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,337 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:144.25|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:7ba2dd0c-8372-4456-8dd5-c94c670bf771,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,338 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 146\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,339 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 156\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,340 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,340 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,341 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,439 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926273439\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,446 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,577 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 136\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,577 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:134.64|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:4ff36f12-e456-4d0f-93b8-c5cc41723102,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,578 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 152\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,579 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,580 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,584 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,681 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926273681\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,685 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,801 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:115.43|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:6f5a3777-6ebd-4598-a734-db3d08be9cf6,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,801 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 116\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,802 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 135\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,803 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,804 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,805 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,919 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926273919\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:33,930 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926273\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,042 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 116\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,043 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 128\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,577 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 136\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,577 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:134.64|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:4ff36f12-e456-4d0f-93b8-c5cc41723102,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,578 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 152\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,579 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,580 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,584 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:9|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,681 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926273681\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,685 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,801 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:115.43|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:6f5a3777-6ebd-4598-a734-db3d08be9cf6,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,801 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 116\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,802 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 135\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,803 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,804 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,805 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:8|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,919 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926273919\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:33,930 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926273\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,042 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 116\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,043 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 128\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,043 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,044 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,042 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:115.57|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:a0470b15-b1e6-45e6-a4a9-251db29c3a7c,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,045 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:10|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,115 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926274115\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,118 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,239 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 122\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,240 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 137\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,239 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:120.29|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:86bb02f6-2943-44da-9e85-1d752f1c8ea8,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,240 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,043 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,044 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,042 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:115.57|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:a0470b15-b1e6-45e6-a4a9-251db29c3a7c,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,045 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:10|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,115 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926274115\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,118 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,239 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 122\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,240 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 137\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,239 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:120.29|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:86bb02f6-2943-44da-9e85-1d752f1c8ea8,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,240 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,240 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,241 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,350 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926274350\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,358 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,240 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,241 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:3|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,350 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926274350\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,358 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,485 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:131.51|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:cdc0b765-7029-4b71-84bf-542a69fd5aa2,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,485 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 133\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,486 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 139\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,487 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,488 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,489 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,581 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926274581\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,586 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,700 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 117\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,701 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 134\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,701 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,702 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,705 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,706 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:115.84|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:f427b933-885d-44d9-bdcd-8e21b6a3eeff,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,778 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926274778\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,786 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,916 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 135\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,919 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 153\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,919 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,919 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,920 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[34m2025-06-26T08:24:34,923 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:133.34|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:edc6c9a2-afe5-4ba7-9598-c91a5f9ab663,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,485 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:131.51|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:cdc0b765-7029-4b71-84bf-542a69fd5aa2,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,485 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 133\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,486 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 139\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,487 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,488 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,489 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:6|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,581 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926274581\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,586 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,700 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 117\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,701 [INFO ] W-9001-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 134\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,701 [INFO ] W-9001-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,702 [INFO ] W-9001-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,705 [INFO ] W-9001-model_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,706 [INFO ] W-9001-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:115.84|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:f427b933-885d-44d9-bdcd-8e21b6a3eeff,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,778 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1750926274778\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,786 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Backend received inference at: 1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,916 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Backend response time: 135\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,919 [INFO ] W-9000-model_1.0 ACCESS_LOG - /169.254.255.130:40586 \"POST /invocations HTTP/1.1\" 200 153\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,919 [INFO ] W-9000-model_1.0 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926268\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,919 [INFO ] W-9000-model_1.0 TS_METRICS - QueueTime.ms:0|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,920 [INFO ] W-9000-model_1.0 TS_METRICS - WorkerThreadTime.ms:7|#Level:Host|#hostname:2f692db0f27f,timestamp:1750926274\u001b[0m\n",
      "\u001b[35m2025-06-26T08:24:34,923 [INFO ] W-9000-model_1.0-stdout MODEL_METRICS - PredictionTime.Milliseconds:133.34|#ModelName:model,Level:Model|#hostname:2f692db0f27f,requestID:edc6c9a2-afe5-4ba7-9598-c91a5f9ab663,timestamp:1750926274\u001b[0m\n",
      "✓ Batch Transform completed.\n"
     ]
    }
   ],
   "source": [
    "# Start the batch transform job\n",
    "transformer.transform(\n",
    "    data=input_location,\n",
    "    content_type='application/x-image',\n",
    "    split_type='None'\n",
    ")\n",
    "\n",
    "# Wait for the job to complete\n",
    "transformer.wait()\n",
    "print(\"✓ Batch Transform completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8a316-2c22-4d1c-933a-1039c4ca105d",
   "metadata": {},
   "source": [
    "## 3. View and Download Batch Transform Outputs\n",
    "\n",
    "After the batch job completes, we list and download the output prediction files from S3. Each `.out` file contains the model’s predicted probabilities for a single frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b21e82ab-7c9a-4518-9c18-d939153534f0",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-06-26T08:25:57.081930Z",
     "iopub.status.busy": "2025-06-26T08:25:57.081568Z",
     "iopub.status.idle": "2025-06-26T08:25:57.227317Z",
     "shell.execute_reply": "2025-06-26T08:25:57.226670Z",
     "shell.execute_reply.started": "2025-06-26T08:25:57.081906Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_output/\n",
      "batch_output/frame_0000.jpg.out\n",
      "batch_output/frame_0001.jpg.out\n",
      "batch_output/frame_0002.jpg.out\n",
      "batch_output/frame_0003.jpg.out\n",
      "batch_output/frame_0004.jpg.out\n",
      "batch_output/frame_0005.jpg.out\n",
      "batch_output/frame_0006.jpg.out\n",
      "batch_output/frame_0007.jpg.out\n",
      "batch_output/frame_0008.jpg.out\n",
      "batch_output/frame_0009.jpg.out\n",
      "batch_output/frame_0010.jpg.out\n",
      "batch_output/frame_0011.jpg.out\n",
      "batch_output/frame_0012.jpg.out\n",
      "batch_output/frame_0013.jpg.out\n",
      "batch_output/frame_0014.jpg.out\n",
      "batch_output/frame_0015.jpg.out\n",
      "batch_output/frame_0016.jpg.out\n",
      "batch_output/frame_0017.jpg.out\n",
      "batch_output/frame_0018.jpg.out\n",
      "batch_output/frame_0019.jpg.out\n",
      "batch_output/frame_0020.jpg.out\n",
      "batch_output/frame_0021.jpg.out\n"
     ]
    }
   ],
   "source": [
    "#View the Output\n",
    "bucket = 'sagemaker-us-east-1-301806113644'\n",
    "prefix = 'batch_output/'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "for obj in response.get('Contents', []):\n",
    "    print(obj['Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d95c1f00-03d5-45eb-9b3f-c9010a993fac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T08:26:05.795661Z",
     "iopub.status.busy": "2025-06-26T08:26:05.795294Z",
     "iopub.status.idle": "2025-06-26T08:26:07.208003Z",
     "shell.execute_reply": "2025-06-26T08:26:07.207379Z",
     "shell.execute_reply.started": "2025-06-26T08:26:05.795639Z"
    }
   },
   "outputs": [],
   "source": [
    "bucket = 'sagemaker-us-east-1-301806113644'\n",
    "prefix = 'batch_output/'\n",
    "local_folder = './batch_output/'\n",
    "\n",
    "os.makedirs(local_folder, exist_ok=True)\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# List all .out files\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "for obj in response['Contents']:\n",
    "    key = obj['Key']\n",
    "    if key.endswith('.out'):\n",
    "        local_path = os.path.join(local_folder, os.path.basename(key))\n",
    "        s3.download_file(bucket, key, local_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d8a62-27ee-49d3-b3d5-d2cc8bb82c8d",
   "metadata": {},
   "source": [
    "## 4. Post-processing: Parse Batch Predictions\n",
    "\n",
    "We parse each output file to determine the predicted emotion label per frame, creating a structured results DataFrame for further analysis or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e321d78b-8331-4492-ae85-61023baf6a3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T08:26:24.286096Z",
     "iopub.status.busy": "2025-06-26T08:26:24.285756Z",
     "iopub.status.idle": "2025-06-26T08:26:24.312649Z",
     "shell.execute_reply": "2025-06-26T08:26:24.311903Z",
     "shell.execute_reply.started": "2025-06-26T08:26:24.286072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         frame  emotion\n",
      "0   frame_0000  Disgust\n",
      "1   frame_0001    Anger\n",
      "2   frame_0002  Disgust\n",
      "3   frame_0003    Anger\n",
      "4   frame_0004  Disgust\n",
      "5   frame_0005  Disgust\n",
      "6   frame_0006  Disgust\n",
      "7   frame_0007  Disgust\n",
      "8   frame_0008  Disgust\n",
      "9   frame_0009  Disgust\n",
      "10  frame_0010  Disgust\n",
      "11  frame_0011  Disgust\n",
      "12  frame_0012  Disgust\n",
      "13  frame_0013  Disgust\n",
      "14  frame_0014  Disgust\n",
      "15  frame_0015  Disgust\n",
      "16  frame_0016  Disgust\n",
      "17  frame_0017    Anger\n",
      "18  frame_0018  Disgust\n",
      "19  frame_0019  Disgust\n",
      "20  frame_0020  Disgust\n",
      "21  frame_0021    Anger\n"
     ]
    }
   ],
   "source": [
    "emotion_labels = ['Neutral', 'Happiness', 'Surprise', 'Sadness', 'Anger', 'Disgust', 'Fear', 'Contempt']\n",
    "\n",
    "results = []\n",
    "for fname in sorted(os.listdir(local_folder)):\n",
    "    if fname.endswith('.out'):\n",
    "        with open(os.path.join(local_folder, fname), 'r') as f:\n",
    "            preds = json.loads(f.read())\n",
    "            max_idx = int(np.argmax(preds))\n",
    "            label = emotion_labels[max_idx]\n",
    "            results.append({\n",
    "                'frame': fname.replace('.jpg.out', ''),\n",
    "                'emotion': label,\n",
    "                'probabilities': preds\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "print(df[['frame', 'emotion']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c202fb9-826f-4080-b946-77d61fc03d62",
   "metadata": {},
   "source": [
    "## 5. Visualization: Plot Emotions Over Time\n",
    "\n",
    "We visualize the emotion predictions for each video frame, helping to identify patterns or emotional shifts over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fbf0c88-4b72-4be6-84fa-f2272adb73bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T08:26:47.645379Z",
     "iopub.status.busy": "2025-06-26T08:26:47.645060Z",
     "iopub.status.idle": "2025-06-26T08:26:47.846041Z",
     "shell.execute_reply": "2025-06-26T08:26:47.845437Z",
     "shell.execute_reply.started": "2025-06-26T08:26:47.645357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['second'] = df['frame'].str.extract(r'(\\d+)').astype(int)\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.plot(df['second'], df['emotion'], marker='o')\n",
    "plt.xlabel('Frame/Second')\n",
    "plt.ylabel('Predicted Emotion')\n",
    "plt.title('Emotion per Frame/Second')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9ebfdb-10d3-4d9f-8ff8-ac4950f0b1ed",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates a complete workflow for performing large-scale batch inference using AWS SageMaker Batch Transform, parsing the results, and visualizing emotion recognition outcomes over time. This approach enables scalable, automated post-processing for video-based machine learning pipelines."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
