{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e90176e-4865-4683-bfd3-8ee8ca6a184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1: Set Up Model Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3cb745d-3a65-47de-b693-ebe150426b44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-22T04:12:33.440280Z",
     "iopub.status.busy": "2025-06-22T04:12:33.439926Z",
     "iopub.status.idle": "2025-06-22T04:12:34.163790Z",
     "shell.execute_reply": "2025-06-22T04:12:34.162923Z",
     "shell.execute_reply.started": "2025-06-22T04:12:33.440254Z"
    }
   },
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateModelPackageGroup operation: Model Package Group already exists: arn:aws:sagemaker:us-east-1:877305355343:model-package-group/facial-expression-analysis-aai540",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m group_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacial-expression-analysis-aai540\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMobileNetV2 model to classify facial expressions for neuromarketing using FERPlus and RAF-DB.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model_package_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mModelPackageGroupName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mModelPackageGroupDescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateModelPackageGroup operation: Model Package Group already exists: arn:aws:sagemaker:us-east-1:877305355343:model-package-group/facial-expression-analysis-aai540"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "\n",
    "group_name = \"facial-expression-analysis-aai540\"\n",
    "description = \"MobileNetV2 model to classify facial expressions for neuromarketing using FERPlus and RAF-DB.\"\n",
    "\n",
    "client.create_model_package_group(\n",
    "    ModelPackageGroupName=group_name,\n",
    "    ModelPackageGroupDescription=description\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84f05b5a-3b0c-40d5-837e-dcf7772621ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T01:45:26.488683Z",
     "iopub.status.busy": "2025-06-17T01:45:26.488166Z",
     "iopub.status.idle": "2025-06-17T01:45:26.588982Z",
     "shell.execute_reply": "2025-06-17T01:45:26.588162Z",
     "shell.execute_reply.started": "2025-06-17T01:45:26.488653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ModelPackageGroupName': 'facial-expression-analysis-aai540', 'ModelPackageGroupArn': 'arn:aws:sagemaker:us-east-1:877305355343:model-package-group/facial-expression-analysis-aai540', 'ModelPackageGroupDescription': 'MobileNetV2 model to classify facial expressions for neuromarketing using FERPlus and RAF-DB.', 'CreationTime': datetime.datetime(2025, 6, 17, 1, 44, 36, 82000, tzinfo=tzlocal()), 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:877305355343:user-profile/d-pouurhqhlhjk/default-20250607T190108', 'UserProfileName': 'default-20250607T190108', 'DomainId': 'd-pouurhqhlhjk', 'IamIdentity': {'Arn': 'arn:aws:sts::877305355343:assumed-role/AmazonSageMaker-ExecutionRole-20250607T190109/SageMaker', 'PrincipalId': 'AROA4YQ3ZLRHVDVH366J5:SageMaker'}}, 'ModelPackageGroupStatus': 'Completed', 'ResponseMetadata': {'RequestId': '43130e58-db84-4a31-b0eb-4662fb2cf6b0', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '43130e58-db84-4a31-b0eb-4662fb2cf6b0', 'content-type': 'application/x-amz-json-1.1', 'content-length': '743', 'date': 'Tue, 17 Jun 2025 01:45:26 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = client.describe_model_package_group(ModelPackageGroupName=group_name)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94b4e60-8cd8-443e-aeb6-69fabecf9d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2: Set up Model Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcc99935-f72e-459c-bda1-95a95c6ef48a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T01:49:39.601924Z",
     "iopub.status.busy": "2025-06-17T01:49:39.601051Z",
     "iopub.status.idle": "2025-06-17T01:49:39.985668Z",
     "shell.execute_reply": "2025-06-17T01:49:39.984954Z",
     "shell.execute_reply.started": "2025-06-17T01:49:39.601894Z"
    }
   },
   "outputs": [],
   "source": [
    "model_package_response = client.create_model_package(\n",
    "    ModelPackageGroupName=group_name,\n",
    "    ModelPackageDescription=\"Facial expression analysis model using MobileNetV2.\",\n",
    "    InferenceSpecification={\n",
    "        'Containers': [{\n",
    "            'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.9.1-cpu-py38-ubuntu20.04',\n",
    "            'ModelDataUrl': 's3://sagemaker-us-east-1-877305355343/mobV2_model.tar.gz',\n",
    "            'Environment': {}\n",
    "        }],\n",
    "        'SupportedContentTypes': ['application/x-image'],\n",
    "        'SupportedResponseMIMETypes': ['application/json']\n",
    "    },\n",
    "    CertifyForMarketplace=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b6a7f8-cd9f-4b67-ab93-766e38bf2037",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T01:49:51.406569Z",
     "iopub.status.busy": "2025-06-17T01:49:51.405876Z",
     "iopub.status.idle": "2025-06-17T01:49:51.458285Z",
     "shell.execute_reply": "2025-06-17T01:49:51.457646Z",
     "shell.execute_reply.started": "2025-06-17T01:49:51.406540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ModelPackageGroupName': 'facial-expression-analysis-aai540', 'ModelPackageVersion': 1, 'ModelPackageArn': 'arn:aws:sagemaker:us-east-1:877305355343:model-package/facial-expression-analysis-aai540/1', 'ModelPackageDescription': 'Facial expression analysis model using MobileNetV2.', 'CreationTime': datetime.datetime(2025, 6, 17, 1, 49, 39, 959000, tzinfo=tzlocal()), 'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.9.1-cpu-py38-ubuntu20.04', 'ImageDigest': 'sha256:327d9112f93764b3d94edf7e2629d9a4d4dc410ccce0e7d8e3a8f4b4d9cdc91c', 'ModelDataUrl': 's3://sagemaker-us-east-1-877305355343/mobV2_model.tar.gz', 'Environment': {}, 'ModelDataETag': 'e2f76cad2c8e785ec046b3d57528f9a6'}], 'SupportedContentTypes': ['application/x-image'], 'SupportedResponseMIMETypes': ['application/json']}, 'ModelPackageStatus': 'Completed', 'ModelPackageStatusDetails': {'ValidationStatuses': [], 'ImageScanStatuses': []}, 'CertifyForMarketplace': False, 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:877305355343:user-profile/d-pouurhqhlhjk/default-20250607T190108', 'UserProfileName': 'default-20250607T190108', 'DomainId': 'd-pouurhqhlhjk', 'IamIdentity': {'Arn': 'arn:aws:sts::877305355343:assumed-role/AmazonSageMaker-ExecutionRole-20250607T190109/SageMaker', 'PrincipalId': 'AROA4YQ3ZLRHVDVH366J5:SageMaker'}}, 'ResponseMetadata': {'RequestId': 'f798c1b4-6e41-4b5e-b0b1-53f6c3aee56d', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'f798c1b4-6e41-4b5e-b0b1-53f6c3aee56d', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1285', 'date': 'Tue, 17 Jun 2025 01:49:51 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "model_package_name = model_package_response['ModelPackageArn']\n",
    "\n",
    "response = client.describe_model_package(ModelPackageName=model_package_name)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec4026-5bcc-455e-a2db-e57c179545b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3: Write a Model Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34c54cd8-5035-4a16-8c6b-45001b5f9789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T01:55:16.337736Z",
     "iopub.status.busy": "2025-06-17T01:55:16.337003Z",
     "iopub.status.idle": "2025-06-17T01:55:16.341831Z",
     "shell.execute_reply": "2025-06-17T01:55:16.341212Z",
     "shell.execute_reply.started": "2025-06-17T01:55:16.337711Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "model_card_content = {\n",
    "    \"model_overview\": {\n",
    "        \"model_description\": \"MobileNetV2 model trained to classify facial expressions into 7 emotions using FERPlus and RAF-DB datasets.\",\n",
    "        \"model_owner\": \"Arifa Kokab\",\n",
    "        \"problem_type\": \"Image Classification\",\n",
    "        \"algorithm_type\": \"Convolutional Neural Network\"\n",
    "    },\n",
    "    \"intended_uses\": {\n",
    "        \"intended_uses\": \"Neuromarketing teams will use this model to analyze emotional responses from facial expressions across video content.\"\n",
    "    },\n",
    "    \"training_details\": {\n",
    "        \"training_observations\": \"The model was trained using stratified and balanced data splits from FERPlus and RAF-DB. Image augmentation and class weights were used.\"\n",
    "    },\n",
    "    \"evaluation_details\": [\n",
    "        {\n",
    "            \"name\": \"Validation Accuracy\",\n",
    "            \"evaluation_job_arn\": \"arn:aws:sagemaker:us-east-1:123456789012:transform-job/facial-expression-eval-job\",\n",
    "            \"evaluation_observation\": \"Validation accuracy achieved 89.6% on combined FERPlus and RAF-DB validation set.\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Macro F1 Score\",\n",
    "            \"evaluation_job_arn\": \"arn:aws:sagemaker:us-east-1:123456789012:transform-job/facial-expression-eval-job\",\n",
    "            \"evaluation_observation\": \"Macro average F1 score was 0.831 as calculated using sklearn metrics.\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c77bb5cc-3cd9-46cf-8adf-51a6539a649b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T01:55:30.101241Z",
     "iopub.status.busy": "2025-06-17T01:55:30.100844Z",
     "iopub.status.idle": "2025-06-17T01:55:30.654649Z",
     "shell.execute_reply": "2025-06-17T01:55:30.653951Z",
     "shell.execute_reply.started": "2025-06-17T01:55:30.101209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ModelCardArn': 'arn:aws:sagemaker:us-east-1:877305355343:model-card/facial-expression-analysis-card',\n",
       " 'ResponseMetadata': {'RequestId': '613d3e6e-59cd-4fe7-bcdc-a3644e4a13c4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '613d3e6e-59cd-4fe7-bcdc-a3644e4a13c4',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '102',\n",
       "   'date': 'Tue, 17 Jun 2025 01:55:30 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_model_card(\n",
    "    ModelCardName='facial-expression-analysis-card',\n",
    "    Content=json.dumps(model_card_content),\n",
    "    ModelCardStatus='Draft'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbe2a7e5-33e7-465f-a8f6-a747c0dc59e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T01:55:43.788998Z",
     "iopub.status.busy": "2025-06-17T01:55:43.788623Z",
     "iopub.status.idle": "2025-06-17T01:55:44.052143Z",
     "shell.execute_reply": "2025-06-17T01:55:44.051507Z",
     "shell.execute_reply.started": "2025-06-17T01:55:43.788977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ModelCardArn': 'arn:aws:sagemaker:us-east-1:877305355343:model-card/facial-expression-analysis-card', 'ModelCardName': 'facial-expression-analysis-card', 'ModelCardVersion': 1, 'Content': '{\"model_overview\": {\"model_description\": \"MobileNetV2 model trained to classify facial expressions into 7 emotions using FERPlus and RAF-DB datasets.\", \"model_owner\": \"Arifa Kokab\", \"problem_type\": \"Image Classification\", \"algorithm_type\": \"Convolutional Neural Network\"}, \"intended_uses\": {\"intended_uses\": \"Neuromarketing teams will use this model to analyze emotional responses from facial expressions across video content.\"}, \"training_details\": {\"training_observations\": \"The model was trained using stratified and balanced data splits from FERPlus and RAF-DB. Image augmentation and class weights were used.\"}, \"evaluation_details\": [{\"name\": \"Validation Accuracy\", \"evaluation_job_arn\": \"arn:aws:sagemaker:us-east-1:123456789012:transform-job/facial-expression-eval-job\", \"evaluation_observation\": \"Validation accuracy achieved 89.6% on combined FERPlus and RAF-DB validation set.\"}, {\"name\": \"Macro F1 Score\", \"evaluation_job_arn\": \"arn:aws:sagemaker:us-east-1:123456789012:transform-job/facial-expression-eval-job\", \"evaluation_observation\": \"Macro average F1 score was 0.831 as calculated using sklearn metrics.\"}]}', 'ModelCardStatus': 'Draft', 'CreationTime': datetime.datetime(2025, 6, 17, 1, 55, 30, 366000, tzinfo=tzlocal()), 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:877305355343:user-profile/d-pouurhqhlhjk/default-20250607T190108', 'UserProfileName': 'default-20250607T190108', 'DomainId': 'd-pouurhqhlhjk'}, 'LastModifiedTime': datetime.datetime(2025, 6, 17, 1, 55, 30, 366000, tzinfo=tzlocal()), 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:us-east-1:877305355343:user-profile/d-pouurhqhlhjk/default-20250607T190108', 'UserProfileName': 'default-20250607T190108', 'DomainId': 'd-pouurhqhlhjk'}, 'ResponseMetadata': {'RequestId': '1b2612c3-c831-4ed9-ac4d-d242f57818a9', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1b2612c3-c831-4ed9-ac4d-d242f57818a9', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1862', 'date': 'Tue, 17 Jun 2025 01:55:44 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "response = client.describe_model_card(ModelCardName='facial-expression-analysis-card')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc255206-740a-41e8-ab95-3d59f5ddcd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
