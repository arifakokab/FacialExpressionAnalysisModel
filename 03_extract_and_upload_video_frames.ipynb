{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4084fef4-a3a2-4786-8c98-9e0224c24a3f",
   "metadata": {},
   "source": [
    "---\n",
    "**Author:** Arifa Kokab  \n",
    "**For:** AAI-540 Machine Learning Operations  \n",
    "**Institution:** University of San Diego"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a8b2f-e61a-415d-8c81-59dc002c3e2a",
   "metadata": {},
   "source": [
    "# Video Frame Extraction & Upload Pipeline\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook demonstrates an automated workflow for extracting frames from a video file and uploading them to AWS S3. This process enables downstream machine learning tasks such as batch inference, facial expression analysis, or dataset creation. The workflow is optimized for reproducibility and integration into a larger MLOps pipeline.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd3f590-5ccf-404d-8a1c-0a6195aae61b",
   "metadata": {},
   "source": [
    "## 1. Setup: Import Libraries and Initialize S3 Client\n",
    "\n",
    "This section imports the required libraries (Boto3, OpenCV, and OS) and initializes the AWS S3 client for subsequent file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0d10e00-c1da-461a-b831-62706a79f63d",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-06-26T08:17:19.585738Z",
     "iopub.status.busy": "2025-06-26T08:17:19.585425Z",
     "iopub.status.idle": "2025-06-26T08:17:19.931440Z",
     "shell.execute_reply": "2025-06-26T08:17:19.930801Z",
     "shell.execute_reply.started": "2025-06-26T08:17:19.585715Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eda74a4-23bf-4326-ad71-19f4df2e1219",
   "metadata": {},
   "source": [
    "## 2. Download Video File from S3\n",
    "\n",
    "We specify the S3 bucket and object key (video filename) and download the video file locally. This ensures the video is available for frame extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caed3475-88da-42c9-b42e-196b8220192f",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-06-26T08:17:29.701359Z",
     "iopub.status.busy": "2025-06-26T08:17:29.700960Z",
     "iopub.status.idle": "2025-06-26T08:17:30.205167Z",
     "shell.execute_reply": "2025-06-26T08:17:30.204507Z",
     "shell.execute_reply.started": "2025-06-26T08:17:29.701332Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 'videos/WIN_20250622_02_25_17_Pro.mp4' from S3 bucket 'sagemaker-us-east-1-301806113644' to 'WIN_20250622_02_25_17_Pro.mp4'\n"
     ]
    }
   ],
   "source": [
    "# Specify your bucket name and object key (filename in S3)\n",
    "bucket_name = 'sagemaker-us-east-1-301806113644'\n",
    "object_key = 'videos/WIN_20250622_02_25_17_Pro.mp4'\n",
    "local_file = 'WIN_20250622_02_25_17_Pro.mp4'\n",
    "\n",
    "# Download from S3\n",
    "s3.download_file(bucket_name, object_key, local_file)\n",
    "print(f\"Downloaded '{object_key}' from S3 bucket '{bucket_name}' to '{local_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fff354-0dad-4803-b4f1-caf8e5dee0a5",
   "metadata": {},
   "source": [
    "## 3. Extract Frames from Video\n",
    "\n",
    "Using OpenCV, we extract frames from the video at a fixed frame rate (e.g., 1 frame per second). Each extracted frame is saved as a JPEG image in a local directory. This allows for granular analysis or batch processing in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9b084be-c466-4c67-9668-267a3a8a39bb",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2025-06-26T08:17:40.660060Z",
     "iopub.status.busy": "2025-06-26T08:17:40.659722Z",
     "iopub.status.idle": "2025-06-26T08:17:47.978206Z",
     "shell.execute_reply": "2025-06-26T08:17:47.977436Z",
     "shell.execute_reply.started": "2025-06-26T08:17:40.660036Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 22 frames extracted to 'video_frames'\n"
     ]
    }
   ],
   "source": [
    "video_path = \"WIN_20250622_02_25_17_Pro.mp4\"\n",
    "output_dir = \"video_frames\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_rate = 1  # Extract 1 frame per second\n",
    "frame_count = 0\n",
    "saved_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    if int(cap.get(cv2.CAP_PROP_POS_FRAMES)) % int(cap.get(cv2.CAP_PROP_FPS) * frame_rate) == 0:\n",
    "        frame_filename = os.path.join(output_dir, f\"frame_{saved_count:04d}.jpg\")\n",
    "        cv2.imwrite(frame_filename, frame)\n",
    "        saved_count += 1\n",
    "cap.release()\n",
    "\n",
    "print(f\"✓ {saved_count} frames extracted to '{output_dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7a7ee6-563e-49d5-a5bb-08f1e6baea84",
   "metadata": {},
   "source": [
    "## 4. Upload Extracted Frames to S3\n",
    "\n",
    "All extracted frame images are uploaded to a specified folder (prefix) in the same S3 bucket. This step enables distributed processing, storage, or further machine learning tasks on AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "415efbaf-c18a-4c91-af38-9054b9055664",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-26T08:18:02.406594Z",
     "iopub.status.busy": "2025-06-26T08:18:02.406267Z",
     "iopub.status.idle": "2025-06-26T08:18:04.640581Z",
     "shell.execute_reply": "2025-06-26T08:18:04.639902Z",
     "shell.execute_reply.started": "2025-06-26T08:18:02.406573Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All frames uploaded to S3.\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'sagemaker-us-east-1-301806113644'\n",
    "s3_prefix = 'batch_input/'\n",
    "\n",
    "for filename in os.listdir(output_dir):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        local_path = os.path.join(output_dir, filename)\n",
    "        s3.upload_file(local_path, bucket_name, s3_prefix + filename)\n",
    "\n",
    "print(\"✓ All frames uploaded to S3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ee1cf-59c7-4240-a3af-89dd957268f6",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a reproducible method for extracting video frames and storing them in AWS S3, forming a crucial preprocessing step in video-based machine learning pipelines. The approach ensures data is efficiently prepared for batch inference, annotation, or training purposes within a cloud-based MLOps workflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
